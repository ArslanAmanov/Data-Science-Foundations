{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Storage & File Formats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python has become a beloved language for text and file munging due to its simple syntax\n",
    "# for interacting with files, intuitive data structures, and convenient features like tuple\n",
    "# packing and unpacking. \n",
    "# Function Description\n",
    "# read_csv >>>>> Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\n",
    "# read_table >>>>> Load delimited data from a file, URL, or file-like object. Use tab ('\\t') as default delimiter\n",
    "# read_fwf >>>>>> Read data in fixed-width column format (that is, no delimiters)\n",
    "# read_clipboard >>>>> Version of read_table that reads data from the clipboard. Useful for converting tables from web pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('examples/ex1.csv') # read_csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also have used read_table and specified the delimiter: \n",
    "# df = pd.read_table('examples/ex1.csv', sep=',') # read_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A file will not always have a header row. Consider this file: \n",
    "# To read this file, you have a couple of options. You can allow pandas to assign default\n",
    "# column names, or you can specify names yourself:\n",
    "# df = pd.read_csv('examples/ex2.csv', header=None) # read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message']) # read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you wanted the message column to be the index of the returned DataFrame. \n",
    "# You can either indicate you want the column at index 4 or names 'message' using the index_col argument: \n",
    "names = ['a', 'b', 'c', 'd', 'message'] \n",
    "# df = pd.read_csv('examples/ex2.csv', names=names, index_col='message') # read_csv with names and index_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the event that you want to form a hierarchical index from multiple columns, pass a list of column numbers or names:\n",
    "# parsed = pd.read_csv('examples/csv_mindex.csv', index_col=['key1', 'key2']) # read_csv with hierarchical index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some cases a table might not have a fixed delimiter, using whitespace or some other pattern to separate fields.\n",
    "# In these cases, you can pass a regular expression as a delimiter for read_table.\n",
    "# Consider a text file that looks like this: \n",
    "# list(open('examples/ex3.txt')) # read_table with regular expression as delimiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While you could do some munging by hand, the fields are separated by a variable amount of whitespace. \n",
    "# In these cases, you can pass a regular expression as a delimiter for read_table. \n",
    "# This can be expressed by the regular expression \\s+, so we have then: \n",
    "# result = pd.read_table('examples/ex3.txt', sep='\\s+') # read_table with regular expression as delimiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there was one fewer column name than the number of data rows, \n",
    "# read_table infers that the first column should be the DataFrame's index in this special case. \n",
    "# You can explicitly indicate that you want the first column of data to be the index using the index_col argument: \n",
    "# result = pd.read_table('examples/ex3.txt', sep='\\s+', index_col=[0, 1]) \n",
    "# # read_table with regular expression as delimiter and hierarchical index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hey \n",
    "# skiprows allows you to skip over rows, either at the beginning, end, or at specific indices.\n",
    "# For example, we might want to skip the first, third, and fourth rows of a file like this:\n",
    "# list(open('examples/ex4.csv')) # read_csv with skiprows\n",
    "# df = pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3]) # read_csv with skiprows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values is an important and frequently nuanced part of the file parsing process. \n",
    "# Missing data is usually either not present (empty string) or marked by some sentinel value \n",
    "# Most tabular data formats represent missing data as NA:\n",
    "# result = pd.read_csv('examples/ex5.csv') # read_csv with missing values \n",
    "# result = pd.read_csv('examples/ex5.csv', na_values=['NULL']) # read_csv with missing values and na_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different NA sentinels can be specified for each column in a dict: \n",
    "# sentinels = {'message': ['foo', 'NA'], 'something': ['two']} \n",
    "# result = pd.read_csv('examples/ex5.csv', na_values=sentinels) # read_csv with missing values and na_values in dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Argument Description\n",
    "# path >>>> String indicating filesystem location, URL, or file-like object\n",
    "# sep or delimiter >>>>> Character sequence or regular expression to use to split fields in each row\n",
    "# header >>>>> Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header row\n",
    "# index_col >>>> Column numbers or names to use as the row index in the result. Can be a single name/number or a list \n",
    "# of them for a hierarchical index\n",
    "\n",
    "# names >>>>> List of column names for result, combine with header=None\n",
    "# skiprows >>>>>> Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\n",
    "# na_values >>>>>> Sequence of values to replace with NA\n",
    "# comment >>>>> Character or characters to split comments off the end of lines\n",
    "# parse_dates >>> Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise\n",
    "# can specify a list of column numbers or name to parse. If element of list is tuple or list, will combine\n",
    "# multiple columns together and parse to date (for example if date/time split across two columns)\n",
    "\n",
    "# keep_date_col >>>> If joining columns to parse date, drop the joined columns. Default True\n",
    "# converters >>> Dict containing column number of name mapping to functions. For example {'foo': f} would apply\n",
    "# the function f to all values in the 'foo' column\n",
    "# dayfirst >>>>> When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,\n",
    "# 2012). Default False\n",
    "# date_parser >>>> Function to use to parse dates \n",
    "# nrows >>>> Number of rows to read from beginning of file\n",
    "# iterator >>>>>  Return a TextParser object for reading file piecemeal\n",
    "# chunksize >>> For iteration, size of file chunks\n",
    "# skip_footer >>>>> Number of lines to ignore at end of file\n",
    "# verbose >>>>> Print various parser output information, like the number of missing values placed in non-numeric data\n",
    "# encoding >>>>> Text encoding for Unicode (e.g. 'utf-8' for UTF-8 encoded text)\n",
    "# squeeze >>>> If the parsed data only contains one column, return a Series\n",
    "# thousands >>>>> Separator for thousands (e.g. ',' or '.')\n",
    "# decimal >>>> Character to recognize as decimal point (e.g. ',' or '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Text Files in Pieces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing large files or figuring out the right set of arguments to correctly process a large file, \n",
    "# you may only want to read in a small piece of a file or iterate through smaller chunks of the file. \n",
    "# Before we look at a large file, we make the pandas display settings more compact: \n",
    "# pd.options.display.max_rows = 10  # read_csv with nrows \n",
    "# result = pd.read_csv('examples/ex6.csv') # read_csv with nrows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to only read out a small number of rows (avoiding reading the entire file), specify that with nrows:\n",
    "# result = pd.read_csv('examples/ex6.csv', nrows=5) # read_csv with nrows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read a file in pieces, specify a chunksize as number of rows:\n",
    "# chunker = pd.read_csv('examples/ex6.csv', chunksize=1000) # read_csv with chunksize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TextParser object returned by read_csv allows you to iterate over the parts of the file according to the chunksize.\n",
    "# For example, we can iterate over ex6.csv, aggregating the value counts in the 'key' column like so: \n",
    "# tot = pd.Series([])\n",
    "# for piece in chunker:\n",
    "#     tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "# tot = tot.sort_values(ascending=False) # read_csv with chunksize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data Out to Text Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data can also be exported to a delimited format. Let's  consider one of the CSV files read before: \n",
    "# data = pd.read_csv('examples/ex5.csv') # read_csv \n",
    "# data.to_csv('examples/out.csv') # to_csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame's to_csv method, we can write the data out to a comma-separated file: \n",
    "# import sys\n",
    "# data.to_csv(sys.stdout, sep='|') # to_csv with sys.stdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misssing values appear as empty strings in the output. You might want to denote them by some other sentinel value:\n",
    "# data.to_csv( na_rep='NULL') # to_csv with sys.stdout and na_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With no other options specified, both the row and column labels are written. Both of these can be disabled:\n",
    "# data.to_csv(sys.stdout, index=False, header=False) # to_csv with sys.stdout and index=False and header=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also write only a subset of the columns, and in an order your choosing: \n",
    "# data.to_csv(index=False, columns=['a', 'b', 'c']) # to_csv with sys.stdout and index=False and columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON (JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request\n",
    "# between web browsers and other applications.\n",
    "# It is a much more free-form data format than a tabular text form like CSV. Here is an example:\n",
    "obj = \"\"\" {\n",
    "    \"name\": \"Wes\",\n",
    "    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    "    \"pet\": null,\n",
    "    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},\n",
    "                {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]\n",
    "} \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Wes',\n",
       " 'places_lived': ['United States', 'Spain', 'Germany'],\n",
       " 'pet': None,\n",
       " 'siblings': [{'name': 'Scott', 'age': 25, 'pet': 'Zuko'},\n",
       "  {'name': 'Katie', 'age': 33, 'pet': 'Cisco'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JSON is very nearly valid Python code with the exception of its null value null and some other nuances \n",
    "# like disallowing trailing commas at the end of lists. The basic types are objects (dicts), arrays (lists), \n",
    "# strings, numbers, booleans, and nulls. All of the keys in an object must be strings. There are several \n",
    "# Python libraries for reading and writing JSON data. I'll use json here, as it is built into the Python\n",
    "# standard library. To convert a JSON string to Python form, use json.loads:\n",
    "import json\n",
    "result = json.loads(obj) # json.loads\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"Wes\", \"places_lived\": [\"United States\", \"Spain\", \"Germany\"], \"pet\": null, \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"}, {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json.dumps, on the other hand, converts a Python object back to JSON:\n",
    "json.dumps(result) # json.dumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result) # json.dumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. \n",
    "# Conveniently, you can pass a list of JSON objects to the Dataframe constructor and select a subset of the data fields:\n",
    "import pandas as pd  \n",
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age']) # json.dumps with DataFrame constructor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scott</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Katie</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Scott   25\n",
       "1  Katie   33"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML and HTML: Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install lxml # install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install urllib2 # install urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install beautifulsoup4 html5lib # install beautifulsoup4 html5lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urlopen from urllib2 \n",
    "# from urllib2 import urlopen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests # install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening the URL: http://finance.yahoo.com/q/op?s=AAPL+Options\n",
      "HTTP Error code: 404\n"
     ]
    }
   ],
   "source": [
    "from lxml.html import parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "url = 'http://finance.yahoo.com/q/op?s=AAPL+Options'\n",
    "\n",
    "try:\n",
    "    parsed = parse(urlopen(url))\n",
    "    doc = parsed.getroot()\n",
    "    # Rest of your code here\n",
    "except HTTPError as e:\n",
    "    print(f\"Error opening the URL: {url}\")\n",
    "    print(f\"HTTP Error code: {e.code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing the URL: http://finance.yahoo.com/q/op?s=AAPL+Options\n",
      "Error details: 404 Client Error: Not Found for url: https://finance.yahoo.com/q/op?s=AAPL+Options\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'http://finance.yahoo.com/q/op?s=AAPL+Options'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    parsed = html.fromstring(response.text)\n",
    "    # Rest of your code here\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error accessing the URL: {url}\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
